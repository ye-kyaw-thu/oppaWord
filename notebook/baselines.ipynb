{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed7b9ac-4389-4b26-961f-240e8635687a",
   "metadata": {},
   "source": [
    "## Word Segmentation Baselines\n",
    "\n",
    "ဒီ Notebook မှာ လက်ရှိ LU Lab. ရဲ့ myTokenizers က support လုပ်တဲ့ word segmentation method သုံးမျိုးကို သုံးပြီး baseline ထုတ်ကြည့်မယ်။  \n",
    "\n",
    "For Internship-3 of LU Lab Students.  \n",
    "Prepared by Ye, LU Lab., Myanmar.  \n",
    "Date: 26 July 2025  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbab05d9-8968-4332-b13a-25450ed384ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/myTokenizer\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0632753a-8f6d-4a66-98e0-f3468b6d7d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.8\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ee5df0-1444-41fd-b63e-5098746c9e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/miniforge3/envs/myTokenize/bin/\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "839a4a7a-486c-428a-a056-d402d013da34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/ye/miniforge3/envs/myTokenize:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                 conda_forge    conda-forge\n",
      "_openmp_mutex             4.5                       2_gnu    conda-forge\n",
      "absl-py                   2.3.1                    pypi_0    pypi\n",
      "astunparse                1.6.3                    pypi_0    pypi\n",
      "bzip2                     1.0.8                h4bc722e_7    conda-forge\n",
      "ca-certificates           2025.7.14            hbd8a1cb_0    conda-forge\n",
      "cachetools                5.5.2                    pypi_0    pypi\n",
      "certifi                   2025.7.14                pypi_0    pypi\n",
      "charset-normalizer        3.4.2                    pypi_0    pypi\n",
      "contourpy                 1.3.2                    pypi_0    pypi\n",
      "cycler                    0.12.1                   pypi_0    pypi\n",
      "flatbuffers               25.2.10                  pypi_0    pypi\n",
      "fonttools                 4.59.0                   pypi_0    pypi\n",
      "gast                      0.4.0                    pypi_0    pypi\n",
      "gitdb                     4.0.12                   pypi_0    pypi\n",
      "gitpython                 3.1.41                   pypi_0    pypi\n",
      "google-auth               2.40.3                   pypi_0    pypi\n",
      "google-auth-oauthlib      1.0.0                    pypi_0    pypi\n",
      "google-pasta              0.2.0                    pypi_0    pypi\n",
      "grpcio                    1.74.0                   pypi_0    pypi\n",
      "h5py                      3.14.0                   pypi_0    pypi\n",
      "icu                       75.1                 he02047a_0    conda-forge\n",
      "idna                      3.10                     pypi_0    pypi\n",
      "keras                     2.13.1                   pypi_0    pypi\n",
      "kiwisolver                1.4.8                    pypi_0    pypi\n",
      "ld_impl_linux-64          2.44                 h1423503_1    conda-forge\n",
      "libclang                  18.1.1                   pypi_0    pypi\n",
      "libffi                    3.4.6                h2dba641_1    conda-forge\n",
      "libgcc                    15.1.0               h767d61c_3    conda-forge\n",
      "libgcc-ng                 15.1.0               h69a702a_3    conda-forge\n",
      "libgomp                   15.1.0               h767d61c_3    conda-forge\n",
      "liblzma                   5.8.1                hb9d3cd8_2    conda-forge\n",
      "liblzma-devel             5.8.1                hb9d3cd8_2    conda-forge\n",
      "libnsl                    2.0.1                hb9d3cd8_1    conda-forge\n",
      "libsqlite                 3.50.3               hee844dc_1    conda-forge\n",
      "libstdcxx                 15.1.0               h8f9b012_3    conda-forge\n",
      "libstdcxx-ng              15.1.0               h4852527_3    conda-forge\n",
      "libuuid                   2.38.1               h0b41bf4_0    conda-forge\n",
      "libzlib                   1.3.1                hb9d3cd8_2    conda-forge\n",
      "markdown                  3.8.2                    pypi_0    pypi\n",
      "markupsafe                3.0.2                    pypi_0    pypi\n",
      "matplotlib                3.7.4                    pypi_0    pypi\n",
      "ml-dtypes                 0.2.0                    pypi_0    pypi\n",
      "mytokenize                0.1.1                    pypi_0    pypi\n",
      "ncurses                   6.5                  h2d0b736_3    conda-forge\n",
      "numpy                     1.24.3                   pypi_0    pypi\n",
      "oauthlib                  3.3.1                    pypi_0    pypi\n",
      "openssl                   3.5.1                h7b32b05_0    conda-forge\n",
      "opt-einsum                3.4.0                    pypi_0    pypi\n",
      "packaging                 25.0                     pypi_0    pypi\n",
      "pillow                    11.3.0                   pypi_0    pypi\n",
      "pip                       25.1.1             pyh8b19718_0    conda-forge\n",
      "protobuf                  4.25.8                   pypi_0    pypi\n",
      "pyasn1                    0.6.1                    pypi_0    pypi\n",
      "pyasn1-modules            0.4.2                    pypi_0    pypi\n",
      "pyparsing                 3.2.3                    pypi_0    pypi\n",
      "python                    3.10.8          h4a9ceb5_0_cpython    conda-forge\n",
      "python-crfsuite           0.9.9                    pypi_0    pypi\n",
      "python-dateutil           2.9.0.post0              pypi_0    pypi\n",
      "readline                  8.2                  h8c095d6_2    conda-forge\n",
      "requests                  2.32.4                   pypi_0    pypi\n",
      "requests-oauthlib         2.0.0                    pypi_0    pypi\n",
      "rsa                       4.9.1                    pypi_0    pypi\n",
      "sentencepiece             0.2.0                    pypi_0    pypi\n",
      "setuptools                80.9.0             pyhff2d567_0    conda-forge\n",
      "six                       1.17.0                   pypi_0    pypi\n",
      "smmap                     5.0.2                    pypi_0    pypi\n",
      "tensorboard               2.13.0                   pypi_0    pypi\n",
      "tensorboard-data-server   0.7.2                    pypi_0    pypi\n",
      "tensorflow                2.13.0                   pypi_0    pypi\n",
      "tensorflow-addons         0.21.0                   pypi_0    pypi\n",
      "tensorflow-estimator      2.13.0                   pypi_0    pypi\n",
      "tensorflow-io-gcs-filesystem 0.37.1                   pypi_0    pypi\n",
      "termcolor                 3.1.0                    pypi_0    pypi\n",
      "tk                        8.6.13          noxft_hd72426e_102    conda-forge\n",
      "typeguard                 2.13.3                   pypi_0    pypi\n",
      "typing-extensions         4.5.0                    pypi_0    pypi\n",
      "tzdata                    2025b                h78e105d_0    conda-forge\n",
      "urllib3                   2.5.0                    pypi_0    pypi\n",
      "werkzeug                  3.1.3                    pypi_0    pypi\n",
      "wheel                     0.45.1             pyhd8ed1ab_1    conda-forge\n",
      "wrapt                     1.14.1                   pypi_0    pypi\n",
      "xz                        5.8.1                hbcc6ac9_2    conda-forge\n",
      "xz-gpl-tools              5.8.1                hbcc6ac9_2    conda-forge\n",
      "xz-tools                  5.8.1                hb9d3cd8_2    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962d1385-498d-4aa9-a68a-979f79f5678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 01:30:22.735656: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-26 01:30:22.736825: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 01:30:22.757513: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 01:30:22.757927: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-26 01:30:23.157676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790641e-6199-4ea4-8359-6fc4cff80969",
   "metadata": {},
   "source": [
    "## Testing Syllable Segmentation\n",
    "\n",
    "အရင်ဆုံး Library က အလုပ် လုပ်သလားဆိုတာကို စမ်းကြည့်တာပါ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72d84491-90f0-4b8a-9b87-8e9008eeed54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ဗ', 'မာ', 'စ', 'ကား', 'ပြော', 'တတ်', 'လား', '။']\n"
     ]
    }
   ],
   "source": [
    "from myTokenize import SyllableTokenizer\n",
    "\n",
    "tokenizer = SyllableTokenizer()\n",
    "syllables = tokenizer.tokenize(\"ဗမာစကားပြောတတ်လား။\")\n",
    "print(syllables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9ad25-0624-49c9-b110-fe7cdf688014",
   "metadata": {},
   "source": [
    "## Preparing Open-test Data\n",
    "\n",
    "မြန်မာစာအတွက်က manual word segmentation ဖြတ်ထားပြီးသား data ကလည်း မရှိသလောက်ပဲဆိုတော့...  \n",
    "myPOS (version 3.0) ရဲ့ tag မပါတဲ့ open-test dataset ကိုပဲ word segmentation အတွက်လည်း သုံးမယ်။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f709cad-3b28-4646-a1e4-0d32d1cc6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4a11d8-5970-48ad-bf3b-2d9c6dca700e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/myTokenizer/data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d472df-8a6a-4fd4-9276-64e2f79727b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-26 01:34:46--  https://raw.githubusercontent.com/ye-kyaw-thu/myPOS/refs/heads/master/corpus-ver-3.0/corpus/otest.1k.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 229758 (224K) [text/plain]\n",
      "Saving to: ‘otest.1k.txt’\n",
      "\n",
      "otest.1k.txt        100%[===================>] 224.37K   402KB/s    in 0.6s    \n",
      "\n",
      "2025-07-26 01:34:48 (402 KB/s) - ‘otest.1k.txt’ saved [229758/229758]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ye-kyaw-thu/myPOS/refs/heads/master/corpus-ver-3.0/corpus/otest.1k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b437966-53e8-4f37-b0c7-d591f9b8b6e3",
   "metadata": {},
   "source": [
    "**data မှာက POS Tag တွေ ပါနေသေးတယ်။**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72becc1c-e4ac-4700-9733-fd7dc0eaa6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "တစ်/tn ကိုက်/n ကို/ppm ဝမ်/n ခုနှစ်ထောင်/tn ပါ/part ။/punc\n",
      "မနှစ်/n က/ppm သူ/pron ကျွန်မ/pron ကို/ppm သင်/v ပေး/part တယ်/ppm ။/punc\n",
      "ကျွန်တော့်/pron ခုံ/n သွား/v ရှာ/v မလို့/part ။/punc\n",
      "အတန်း/n စ/v တာ/part ကြာ/v ပြီ/ppm လား/part ။/punc\n",
      "ဆေး/n နည်းနည်း/adv စား/v လိုက်/part ၊/punc သုံး/tn လေး/tn ရက်/n လောက်/part အနားယူ/v လိုက်/part ရင်/conj ပျောက်/v သွား/part မှာ/ppm ပါ/part ။/punc\n",
      "အေးချမ်း/v|မှု/part နဲ့/conj စည်းကမ်း/n ကို/ppm တည်မြဲ/v အောင်/part ထိန်းသိမ်း/v သည်/ppm ။/punc\n",
      "ဇွန်း/n ကို/ppm လိုအပ်/v တယ်/ppm ။/punc\n",
      "ဘွဲ့/n ရ/v ရင်/conj ဘာ/n လုပ်/v မ/part လို့/part လဲ/part ။/punc\n",
      "ကျွန်တော်/pron ချောင်းဆိုး/v ခြင်း/part အတွက်/ppm တစ်/tn ခု/part ခု/part လို/v ချင်/part တယ်/ppm ။/punc\n",
      "အသီးအနှံ/n တို့/part မှ/ppm လွဲ/v လျှင်/conj လူ/n တို့/part ၏/ppm အဓိက/n အစားအစာ/n မှာ/ppm ငါး/n ဖြစ်/v သည်/ppm ။/punc\n"
     ]
    }
   ],
   "source": [
    "!head otest.1k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88d32f-c95b-4300-a1dd-a130cff71076",
   "metadata": {},
   "source": [
    "## Removing POS Tags\n",
    "\n",
    "Tag တွေ၊ word တွေချည်းပဲ ဆွဲထုတ်ပေးတဲ့ perl script ကို ကိုယ့်စက်ထဲကို download လုပ်ယူမယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c59fad0-edc6-4e9d-9bd9-bff54ec07293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-26 01:37:52--  https://raw.githubusercontent.com/ye-kyaw-thu/myPOS/refs/heads/master/corpus-draft-ver-1.0/mk-wordtag.pl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3967 (3.9K) [text/plain]\n",
      "Saving to: ‘mk-wordtag.pl’\n",
      "\n",
      "mk-wordtag.pl       100%[===================>]   3.87K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-07-26 01:37:53 (49.5 MB/s) - ‘mk-wordtag.pl’ saved [3967/3967]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ye-kyaw-thu/myPOS/refs/heads/master/corpus-draft-ver-1.0/mk-wordtag.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d633c5-89e8-4e69-ae30-38ec85c1f563",
   "metadata": {},
   "source": [
    "**Perl code ကို လေ့လာကြည့်ပါ။**  \n",
    "ဖိုင်နာမည်က mk-wordtag.pl ပါ။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c248376-f122-4357-80c6-4b8204254cd1",
   "metadata": {},
   "source": [
    "```perl\n",
    "#!/usr/bin/perl\n",
    "use warnings;\n",
    "use utf8;\n",
    "\n",
    "#last updated: 16 May 2017\n",
    "#written by Ye, AI Lab.,\n",
    "#Okayama Prefectural University, Japan\n",
    "#How to run: perl mk-wordtag.pl <input-file-name> <delimeter> <w|t|wt|cw|c> \n",
    "#Here, \n",
    "# w = print word only (i.e. without POS tags), \n",
    "# t = print tag only\n",
    "# wt = print word/tag\n",
    "# cw = print words including compound words,\n",
    "# lcw = list compound words, \n",
    "# c = print sentence that contain tagging error of \"word/\" \n",
    "# \n",
    "# How to run:\n",
    "# e.g. ./mk-wordtag.pl ./kh-pos.all.f2.utf8 \"\\/\" w | less -r\n",
    "# e.g ./mk-wordtag.pl ./kh-pos.all.f2.utf8 \"\\/\" t\n",
    "# e.g ./mk-wordtag.pl ./kh-pos.all.f2.utf8 \"\\/\" wt\n",
    "\n",
    "binmode STDIN,  \":utf8\";\n",
    "binmode STDOUT, \":utf8\";\n",
    "\n",
    "my $TagMarker=$ARGV[1]; # give command line parameter such as \"\\|\", \"\\/\" ...\n",
    "my $word_or_tag=$ARGV[2];\n",
    "\n",
    "open (my $inputFILE,\"<:encoding(utf8)\", $ARGV[0]) or die \"Couldn't open input file $ARGV[0]!, $!\\n\";\n",
    "\n",
    "my $one_token; my $tmpLine=\"\"; my $tmpLine2=\"\";\n",
    "\n",
    "   while($line = <$inputFILE>)\n",
    "   {\n",
    "      if ($line!~/^$/)\n",
    "      {\n",
    "         chomp ($line);\n",
    "         my $originalLine = $line;\n",
    "         #print $line, \"\\n\";\n",
    "            \n",
    "         $line =~ s/\\s+/ /g;\n",
    "         $line =~ s/^\\s+|\\s+$//g;\n",
    "         if ($word_or_tag eq \"w\" || $word_or_tag eq \"t\" || $word_or_tag eq \"wt\" || $word_or_tag eq \"c\")\n",
    "         {\n",
    "            $line =~ s/\\|/ /g;\n",
    "         }\n",
    "\n",
    "         my @token = split('\\s', $line);\n",
    "         #print \"\\@tokens:\\n\".\"@token\\n\"; \n",
    "         foreach $one_token(@token)\n",
    "         {\n",
    "            #print \"one_token: $one_token\\n\";\n",
    "            my ($text, $tag) = split(/$TagMarker/, $one_token);\n",
    "            if($word_or_tag eq \"w\")\n",
    "            {\n",
    "               $tmpLine = $tmpLine.$text.\" \";\n",
    "            }elsif($word_or_tag eq \"t\")\n",
    "            {\n",
    "               $tmpLine = $tmpLine.$tag.\" \";\n",
    "            }elsif($word_or_tag eq \"wt\" || $word_or_tag eq \"c\")\n",
    "            {\n",
    "               $tmpLine = $tmpLine.$text.\" \";\n",
    "               $tmpLine2 = $tmpLine2.$tag.\" \";\n",
    "            }elsif($word_or_tag eq \"lcw\" || $word_or_tag eq \"cw\")\n",
    "            {\n",
    "               if($one_token =~ m/\\|/g)\n",
    "               { \n",
    "                  my @ptoken = split('\\|', $one_token); my $combined_cword;\n",
    "                     foreach my $cword(@ptoken)\n",
    "                     {\n",
    "                        $cword =~ s/\\/[a-zA-Z].*//g;\n",
    "                        $combined_cword = $combined_cword.$cword;\n",
    "                     }\n",
    "                        if ($word_or_tag eq \"lcw\")\n",
    "                        {\n",
    "                           print \"$one_token\\t$combined_cword\\n\"; #for lcw option;\n",
    "                        }\n",
    "                   $tmpLine = $tmpLine.$combined_cword.\" \"; #for cw option\n",
    "                }elsif($one_token !~ m/\\|/g)\n",
    "                {\n",
    "\n",
    "                   $tmpLine = $tmpLine.$text.\" \";\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "            #chomp($tmpLine);\n",
    "            if ($word_or_tag eq \"w\" || $word_or_tag eq \"t\" || $word_or_tag eq \"cw\")\n",
    "            {\n",
    "               $tmpLine =~ s/^\\s+|\\s+$//g;\n",
    "               print $tmpLine.\"\\n\";\n",
    "            }elsif ($word_or_tag eq \"wt\")\n",
    "            {\n",
    "                \n",
    "               $tmpLine =~ s/^\\s+|\\s+$//g;\n",
    "               $tmpLine2 =~ s/^\\s+|\\s+$//g;\n",
    "               print $tmpLine.\"\\n\"; print $tmpLine2.\"\\n\";\n",
    "            }elsif ($word_or_tag eq \"c\")\n",
    "            {\n",
    "               $tmpLine =~ s/^\\s+|\\s+$//g;\n",
    "               $tmpLine =~ s/\\s+/ /g;\n",
    "               $tmpLine2 =~ s/^\\s+|\\s+$//g;\n",
    "               $tmpLine2 =~ s/\\s+/ /g;\n",
    "               my $word_count = split / /,$tmpLine;\n",
    "               my $tag_count = split / /,$tmpLine2;\n",
    "\n",
    "               if ($word_count != $tag_count)\n",
    "               { \n",
    "                  print \"$originalLine\\n\";\n",
    "                  print \"$word_count: $tag_count\\n\";\n",
    "                  print $tmpLine.\"\\n\"; print $tmpLine2.\"\\n\";\n",
    "               }\n",
    "            \n",
    "             }elsif ($word_or_tag eq \"lcw\")\n",
    "             {\n",
    "               # print $tmpLine2;\n",
    "             }\n",
    "                    $tmpLine = \"\"; $tmpLine2 = \"\";\n",
    "         }\n",
    "      }\n",
    "\n",
    "close($inputFILE);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757e160-c596-492a-9f92-ebb52a8a029d",
   "metadata": {},
   "source": [
    "**word တွေပဲ ဆွဲထုတ်ယူပြီး ဖိုင်အသစ်အနေနဲ့ သိမ်းမယ်...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adf09ea0-75cf-4ea5-87c6-69df27f802e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl ./mk-wordtag.pl ./otest.1k.txt \"\\/\" w > ./otest.1k.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2de21a2-458e-4025-926b-2240892802ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "တစ် ကိုက် ကို ဝမ် ခုနှစ်ထောင် ပါ ။\n",
      "မနှစ် က သူ ကျွန်မ ကို သင် ပေး တယ် ။\n",
      "ကျွန်တော့် ခုံ သွား ရှာ မလို့ ။\n",
      "အတန်း စ တာ ကြာ ပြီ လား ။\n",
      "ဆေး နည်းနည်း စား လိုက် ၊ သုံး လေး ရက် လောက် အနားယူ လိုက် ရင် ပျောက် သွား မှာ ပါ ။\n",
      "အေးချမ်း မှု နဲ့ စည်းကမ်း ကို တည်မြဲ အောင် ထိန်းသိမ်း သည် ။\n",
      "ဇွန်း ကို လိုအပ် တယ် ။\n",
      "ဘွဲ့ ရ ရင် ဘာ လုပ် မ လို့ လဲ ။\n",
      "ကျွန်တော် ချောင်းဆိုး ခြင်း အတွက် တစ် ခု ခု လို ချင် တယ် ။\n",
      "အသီးအနှံ တို့ မှ လွဲ လျှင် လူ တို့ ၏ အဓိက အစားအစာ မှာ ငါး ဖြစ် သည် ။\n"
     ]
    }
   ],
   "source": [
    "!head ./otest.1k.word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00749f7b-a603-4c8c-838f-f13d5425986e",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "myTokenize ကို input လုပ်တဲ့အခါမှာ space တွေအကုန် ဖြုတ်ပေးထားရတာမို့ အဲဒီအတွက် အောက်ပါ python code ကို သုံးမယ်။ \n",
    "ပရိုဂရမ် နာမည်က smart_space_remover.py ပါ။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d3e6a-59d0-4758-8668-33c5f562145d",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "written by Ye Kyaw Thu, LU Lab., Myanmar\n",
    "last updated: 25 July 2025\n",
    "smart_space_remover.py: Remove spaces intelligently for Myanmar text segmentation.\n",
    "\n",
    "Modes:\n",
    "  - all     : Remove all spaces\n",
    "  - my      : Remove spaces only between Myanmar letters\n",
    "  - my_not_num  : Like 'my' but preserve spacing near Myanmar numbers\n",
    "\n",
    "Usage:\n",
    "  $ python smart_space_remover.py --mode my_not_num --input input.txt --output output.txt\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "# === Unicode Character Classes ===\n",
    "MYANMAR_LETTER = r'[\\u1000-\\u109F\\uAA60-\\uAA7F]'\n",
    "MYANMAR_DIGIT = r'[\\u1040-\\u1049]'\n",
    "\n",
    "# === Regex Patterns ===\n",
    "RE_MM_LETTER_SPACE = re.compile(rf'({MYANMAR_LETTER})\\s+({MYANMAR_LETTER})')\n",
    "\n",
    "# Match MyanmarDigit <space> MyanmarDigit\n",
    "RE_MM_DIGIT_DIGIT = re.compile(rf'({MYANMAR_DIGIT})\\s+({MYANMAR_DIGIT})')\n",
    "\n",
    "# Match MyanmarDigit <space> MyanmarLetter\n",
    "RE_MM_DIGIT_LETTER = re.compile(rf'({MYANMAR_DIGIT})\\s+({MYANMAR_LETTER})')\n",
    "\n",
    "# Match MyanmarLetter <space> MyanmarDigit\n",
    "RE_MM_LETTER_DIGIT = re.compile(rf'({MYANMAR_LETTER})\\s+({MYANMAR_DIGIT})')\n",
    "\n",
    "# Protect standalone Myanmar digit tokens and spacing\n",
    "PROTECT_SPACES = [\n",
    "    (RE_MM_DIGIT_DIGIT, r'\\1☃\\2'),           # protect digit-digit\n",
    "    (RE_MM_DIGIT_LETTER, r'\\1☃\\2'),          # protect digit-letter\n",
    "    (RE_MM_LETTER_DIGIT, r'\\1☃\\2'),          # protect letter-digit\n",
    "]\n",
    "\n",
    "def remove_all_spaces(text):\n",
    "    return text.replace(' ', '')\n",
    "\n",
    "def remove_myanmar_spaces(text, preserve_digits=False):\n",
    "    if preserve_digits:\n",
    "        # Step 1: Protect spaces between digits and letters\n",
    "        for pattern, replacement in PROTECT_SPACES:\n",
    "            text = pattern.sub(replacement, text)\n",
    "\n",
    "    # Step 2: Remove space between Myanmar letters\n",
    "    prev = None\n",
    "    while prev != text:\n",
    "        prev = text\n",
    "        text = RE_MM_LETTER_SPACE.sub(r'\\1\\2', text)\n",
    "\n",
    "    if preserve_digits:\n",
    "        # Step 3: Restore protected spaces\n",
    "        text = text.replace('☃', ' ')\n",
    "\n",
    "    return text\n",
    "\n",
    "def process_lines(lines, mode):\n",
    "    for line in lines:\n",
    "        line = line.rstrip('\\n')\n",
    "        if mode == 'all':\n",
    "            yield remove_all_spaces(line)\n",
    "        elif mode == 'my':\n",
    "            yield remove_myanmar_spaces(line, preserve_digits=False)\n",
    "        elif mode == 'my_not_num':\n",
    "            yield remove_myanmar_spaces(line, preserve_digits=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Smart Myanmar space remover\")\n",
    "    parser.add_argument('--mode', choices=['all', 'my', 'my_not_num'], required=True,\n",
    "                        help=\"Mode: 'all', 'my', or 'my_not_num'\")\n",
    "    parser.add_argument('--input', type=str, help=\"Input file (default: stdin)\")\n",
    "    parser.add_argument('--output', type=str, help=\"Output file (default: stdout)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_stream = open(args.input, 'r', encoding='utf-8') if args.input else sys.stdin\n",
    "    output_stream = open(args.output, 'w', encoding='utf-8') if args.output else sys.stdout\n",
    "\n",
    "    try:\n",
    "        for processed in process_lines(input_stream, args.mode):\n",
    "            output_stream.write(processed + '\\n')\n",
    "    finally:\n",
    "        if args.input:\n",
    "            input_stream.close()\n",
    "        if args.output:\n",
    "            output_stream.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7a08e-d85e-4a40-940b-17cadee7daa0",
   "metadata": {},
   "source": [
    "space တွေကို ဖြုတ်မယ်။ --mode ကို my_not_num ထားမယ်။  \n",
    "my_not_num နဲ့ ဆိုရင် မြန်မာ နံပါတ်တွေကို မထိဘူး။ ပြီးတော့ မြန်မာစာ မဟုတ်တဲ့ တခြားဘာသာစကားတွေ ဥပမာ အင်္ဂလိပ်စာ၊ ဂျပန်စာ၊ ထိုင်းစာ တို့ကို space မဖြုတ်ဘူး။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdfb6db7-116d-4d8a-96f0-1984c5d3c89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: smart_space_remover.py [-h] --mode {all,my,my_not_num} [--input INPUT]\n",
      "                              [--output OUTPUT]\n",
      "\n",
      "Smart Myanmar space remover\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --mode {all,my,my_not_num}\n",
      "                        Mode: 'all', 'my', or 'my_not_num'\n",
      "  --input INPUT         Input file (default: stdin)\n",
      "  --output OUTPUT       Output file (default: stdout)\n"
     ]
    }
   ],
   "source": [
    "!python ./smart_space_remover.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "745263a5-82db-4465-8a84-6b1d7c87a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./smart_space_remover.py --input ./otest.1k.word --output ./otest.1k.word.input --mode my_not_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e5e555-40ef-4917-8e51-328c721e8ba0",
   "metadata": {},
   "source": [
    "**space ဖြုတ်ပြီး ထွက်လာတဲ့ ဖိုင်ကို confirm လုပ်ရအောင်**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4ab9815-889a-4d63-9306-730376027dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/myTokenizer/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e718ddf3-c1a8-44ed-87c4-5f205f058885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "တစ်ကိုက်ကိုဝမ်ခုနှစ်ထောင်ပါ။\n",
      "မနှစ်ကသူကျွန်မကိုသင်ပေးတယ်။\n",
      "ကျွန်တော့်ခုံသွားရှာမလို့။\n",
      "အတန်းစတာကြာပြီလား။\n",
      "ဆေးနည်းနည်းစားလိုက်၊သုံးလေးရက်လောက်အနားယူလိုက်ရင်ပျောက်သွားမှာပါ။\n",
      "အေးချမ်းမှုနဲ့စည်းကမ်းကိုတည်မြဲအောင်ထိန်းသိမ်းသည်။\n",
      "ဇွန်းကိုလိုအပ်တယ်။\n",
      "ဘွဲ့ရရင်ဘာလုပ်မလို့လဲ။\n",
      "ကျွန်တော်ချောင်းဆိုးခြင်းအတွက်တစ်ခုခုလိုချင်တယ်။\n",
      "အသီးအနှံတို့မှလွဲလျှင်လူတို့၏အဓိကအစားအစာမှာငါးဖြစ်သည်။\n"
     ]
    }
   ],
   "source": [
    "!head ./otest.1k.word.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04b7d780-165a-4222-9f9e-187c36d5f38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "အိုးခွက်ပန်းကန်တွေသိပ်မရှိလို့ထမင်းဟင်းချက်ရတာအဆင်မပြေဘူး။\n",
      "စိတ်ဝင်စားဖို့ကောင်းတယ်။\n",
      "ဒီဆေးကိုတဝက်စီခွဲပေးပါ။\n",
      "ရောင်းကောင်းလား။\n",
      "ဆရာဒီသွားကခဏခဏနာနေတယ်။\n",
      "အခုဘာလုပ်နေလဲ။\n",
      "ဇူလိုင် ၁၄ ရက်မှာဘန်ကောက်ကိုသွားမယ့် US 123 မှာပါ။ဟုတ်လား။\n",
      "ကားမှကားဘီးကိုဖြုတ်လိုက်သည်။\n",
      "ကျွန်တော်သိပါရစေ။\n",
      "ဘူတာရုံကအလွန်တရာပြည့်ကျပ်နေသည်။\n"
     ]
    }
   ],
   "source": [
    "!tail ./otest.1k.word.input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd56af1a-e0a7-44ff-991f-d849caa359ed",
   "metadata": {},
   "source": [
    "## Coding\n",
    "\n",
    "တကယ်တမ်း လက်တွေ့ word segmentation လုပ်ဖို့က python ပရိုဂရမ် တစ်ပုဒ်ရေးထားလိုက်တာက ပိုအဆင်ပြေတာမို့ အောက်ပါ code ကို ရေးခဲ့တယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcabcda2-de38-4321-bce0-544d0965753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/myTokenizer\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91051579-03e6-4bc7-a355-e21909afc991",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Written by Ye Kyaw Thu., LU Lab., Myanmar.\n",
    "Last updated: 26 July 2025.\n",
    "Usage:\n",
    "time python ./baseline_segmenter.py --input ./data/otest.1k.word.input --output ./output/otest.myword -m myword\n",
    "time python ./baseline_segmenter.py --input ./data/otest.1k.word.input --output ./output/otest.crf -m crf\n",
    "time python ./baseline_segmenter.py --input ./data/otest.1k.word.input --output ./output/otest.lstm -m lstm\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import tensorflow\n",
    "from myTokenize import (\n",
    "    WordTokenizer,\n",
    "    SentenceTokenizer,\n",
    "    SyllableTokenizer,\n",
    "    BPETokenizer,\n",
    "    UnigramTokenizer,\n",
    "    PhraseTokenizer\n",
    ")\n",
    "\n",
    "def get_tokenizer(method):\n",
    "    \"\"\"Return the appropriate tokenizer based on the selected method\"\"\"\n",
    "    method = method.lower()  # Normalize to lowercase\n",
    "    \n",
    "    if method == 'crf':\n",
    "        return WordTokenizer(engine=\"CRF\")\n",
    "    elif method == 'myword':\n",
    "        return WordTokenizer(engine=\"myWord\")  # Note the exact capitalization\n",
    "    elif method == 'lstm':\n",
    "        return WordTokenizer(engine=\"LSTM\")\n",
    "    elif method == 'sentence':\n",
    "        return SentenceTokenizer()\n",
    "    elif method == 'syllable':\n",
    "        return SyllableTokenizer()\n",
    "    elif method == 'bpe':\n",
    "        return BPETokenizer()\n",
    "    elif method == 'unigram':\n",
    "        return UnigramTokenizer()\n",
    "    elif method == 'phrase':\n",
    "        return PhraseTokenizer()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown segmentation method: {method}. Available methods: crf, myword, lstm, sentence, syllable, bpe, unigram, phrase\")\n",
    "\n",
    "def tokenize_text(tokenizer, text):\n",
    "    \"\"\"Tokenize text and return as space-separated string\"\"\"\n",
    "    if not text.strip():\n",
    "        return text  # return empty lines as-is\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text.strip())\n",
    "    \n",
    "    # For all tokenizers except sentence, join with spaces\n",
    "    if not isinstance(tokenizer, SentenceTokenizer):\n",
    "        return ' '.join(tokens)\n",
    "    # For sentence tokenizer, join sentences with newlines\n",
    "    return '\\n'.join(tokens)\n",
    "\n",
    "def process_stream(tokenizer, input_stream, output_stream):\n",
    "    \"\"\"Process input stream and write to output stream\"\"\"\n",
    "    for line in input_stream:\n",
    "        tokenized = tokenize_text(tokenizer, line)\n",
    "        output_stream.write(tokenized + '\\n')\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Myanmar Text Segmenter\",\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-m', '--method',\n",
    "        choices=['crf', 'myword', 'lstm', 'sentence', 'syllable', 'bpe', 'unigram', 'phrase'],\n",
    "        default='crf',\n",
    "        help='Segmentation method to use (crf, myword, lstm, sentence, syllable, bpe, unigram, phrase)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-i', '--input',\n",
    "        type=str,\n",
    "        help='Input file path (default: stdin)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-o', '--output',\n",
    "        type=str,\n",
    "        help='Output file path (default: stdout)'\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Get tokenizer\n",
    "    try:\n",
    "        tokenizer = get_tokenizer(args.method)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Setup input and output streams\n",
    "    input_stream = sys.stdin\n",
    "    if args.input:\n",
    "        try:\n",
    "            input_stream = open(args.input, 'r', encoding='utf-8')\n",
    "        except IOError as e:\n",
    "            print(f\"Error opening input file: {e}\", file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "    \n",
    "    output_stream = sys.stdout\n",
    "    if args.output:\n",
    "        try:\n",
    "            output_stream = open(args.output, 'w', encoding='utf-8')\n",
    "        except IOError as e:\n",
    "            print(f\"Error opening output file: {e}\", file=sys.stderr)\n",
    "            if input_stream != sys.stdin:\n",
    "                input_stream.close()\n",
    "            sys.exit(1)\n",
    "    \n",
    "    # Process the text\n",
    "    try:\n",
    "        process_stream(tokenizer, input_stream, output_stream)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        # Close files if they were opened\n",
    "        if args.input and input_stream != sys.stdin:\n",
    "            input_stream.close()\n",
    "        if args.output and output_stream != sys.stdout:\n",
    "            output_stream.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed2efe-fc8e-4fd1-9769-d9b0d87bdc78",
   "metadata": {},
   "source": [
    "## Word Segmentation with myWord\n",
    "\n",
    "အရင်ဆုံး myWord နဲ့ စာလုံးဖြတ်မယ်။  \n",
    "myWord ရဲ့ word segmentation ဖြတ်ပုံ အသေးစိတ်က GitHub myWord မှာ ဝင်လေ့လာပါ။  \n",
    "Link: [https://github.com/ye-kyaw-thu/myWord](https://github.com/ye-kyaw-thu/myWord)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a07a68c1-8ef3-485f-8d77-1fd3c8fef98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b910671-bb70-46dc-b75b-839ea4528c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 03:07:32.687264: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-26 03:07:32.688214: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 03:07:32.708666: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 03:07:32.708914: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-26 03:07:33.091311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "real\t8m58.617s\n",
      "user\t8m0.665s\n",
      "sys\t1m0.425s\n"
     ]
    }
   ],
   "source": [
    "!time python ./baseline_segmenter.py --input ./data/otest.1k.word.input --output ./output/otest.myword -m myword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24960d33-d0dd-4fc1-912d-b18e6726d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "တစ် ကိုက် ကို ဝမ် ခု နှစ် ထောင် ပါ ။\n",
      "မ နှစ် က သူ ကျွန်မ ကို သင် ပေး တယ် ။\n",
      "ကျွန်တော့် ခုံ သွား ရှာ မ လို့ ။\n",
      "အတန်း စ တာ ကြာ ပြီ လား ။\n",
      "ဆေး နည်းနည်း စား လိုက် ၊ သုံး လေး ရက် လောက် အနားယူ လိုက် ရင် ပျောက် သွား မှာ ပါ ။\n",
      "အေးချမ်း မှု နဲ့ စည်းကမ်း ကို တည်မြဲ အောင် ထိန်းသိမ်း သည် ။\n",
      "ဇွန်း ကို လိုအပ် တယ် ။\n",
      "ဘွဲ့ ရ ရင် ဘာ လုပ် မ လို့ လဲ ။\n",
      "ကျွန်တော် ချောင်းဆိုး ခြင်း အတွက် တစ် ခု ခု လို ချင် တယ် ။\n",
      "အသီးအနှံ တို့ မှ လွဲ လျှင် လူ တို့ ၏ အဓိက အစားအစာ မှာ ငါး ဖြစ် သည် ။\n"
     ]
    }
   ],
   "source": [
    "!head ./output/otest.myword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e3c00ae-5b97-4bd1-b41c-45384087e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "အိုးခွက်ပန်းကန် တွေ သိပ် မ ရှိ လို့ ထမင်း ဟင်း ချက် ရ တာ အဆင်မပြေ ဘူး ။\n",
      "စိတ်ဝင်စား ဖို့ ကောင်း တယ် ။\n",
      "ဒီ ဆေး ကို တဝက် စီ ခွဲ ပေး ပါ ။\n",
      "ရောင်း ကောင်း လား ။\n",
      "ဆရာ ဒီ သွား က ခဏခဏ နာ နေ တယ် ။\n",
      "အခု ဘာ လုပ် နေ လဲ ။\n",
      "ဇူလိုင် ၁၄ ရက် မှာ ဘန်ကောက် ကို သွား မယ် ့US123 မှာ ပါ ။ ဟုတ် လား ။\n",
      "ကား မှ ကား ဘီး ကို ဖြုတ် လိုက် သည် ။\n",
      "ကျွန်တော် သိ ပါ ရ စေ ။\n",
      "ဘူတာရုံ က အလွန်တရာ ပြည့် ကျပ် နေ သည် ။\n"
     ]
    }
   ],
   "source": [
    "!tail ./output/otest.myword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2b5aec7-8a73-40b7-a875-a9ea48ada490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000  14471 181791 ./output/otest.myword\n"
     ]
    }
   ],
   "source": [
    "!wc ./output/otest.myword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb574684-06a3-408c-b77d-05161a80329f",
   "metadata": {},
   "source": [
    "## Word Segmentation with CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4408894-ea08-4e3e-aae2-676b52606e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 03:18:42.946682: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-26 03:18:42.947627: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 03:18:42.968129: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 03:18:42.968376: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-26 03:18:43.350557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "real\t0m1.360s\n",
      "user\t0m1.903s\n",
      "sys\t0m2.000s\n"
     ]
    }
   ],
   "source": [
    "!time python ./baseline_segmenter.py --input ./data/otest.1k.word.input --output ./output/otest.crf -m crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33408d37-bbda-4ee2-a40d-dde1916d1f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "တစ် ကိုက် ကို ဝမ်ခု နှစ်ထောင် ပါ ။\n",
      "မ နှစ် က သူ ကျွန်မ ကို သင်ပေး တယ် ။\n",
      "ကျွန်တော့် ခုံ သွား ရှာ မ လို့ ။\n",
      "အ တန်း စတာ ကြာ ပြီ လား ။\n",
      "ဆေးနည်းနည်းစား လိုက် ၊ သုံး လေးရက်လောက် အနားယူ လိုက် ရင် ပျောက် သွား မှာ ပါ ။\n",
      "အေးချမ်း မှု နဲ့ စည်းကမ်း ကို တည်မြဲ အောင် ထိန်း သိမ်း သည် ။\n",
      "ဇွန်း ကို လို အပ် တယ် ။\n",
      "ဘွဲ့ ရ ရင် ဘာ လုပ် မ လို့ လဲ ။\n",
      "ကျွန်တော်ချောင်း ဆိုးခြင်း အတွက် တစ် ခု ခု လို ချင် တယ် ။\n",
      "အ သီး အ နှံ တို့ မှ လွဲလျှင် လူ တို့ ၏ အဓိက အ စား အစာ မှာ ငါး ဖြစ် သည် ။\n"
     ]
    }
   ],
   "source": [
    "!head ./output/otest.crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44ffffac-6efb-41cf-a419-c2bb6a73819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "အိုးခွက်ပန်းကန် တွေ သိပ် မ ရှိ လို့ ထမင်း ဟင်းချက် ရ တာ အဆင်မ ပြေ ဘူး ။\n",
      "စိတ်ဝင်စား ဖို့ ကောင်း တယ် ။\n",
      "ဒီ ဆေး ကို တဝက်စီ ခွဲ ပေး ပါ ။\n",
      "ရောင်းကောင်း လား ။\n",
      "ဆရာဒီ သွား က ခဏခဏ နာ နေ တယ် ။\n",
      "အခု ဘာ လုပ် နေ လဲ ။\n",
      "ဇူလိုင် ၁၄ ရက် မှာ ဘန်ကောက် ကို သွား မယ့်US123မှာ ပါ ။ဟုတ် လား ။\n",
      "ကား မှ ကားဘီး ကို ဖြုတ် လိုက် သည် ။\n",
      "ကျွန်တော် သိ ပါရစေ ။\n",
      "ဘူတာ ရုံ က အ လွန်တရာ ပြည့် ကျပ် နေ သည် ။\n"
     ]
    }
   ],
   "source": [
    "!tail ./output/otest.crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f1fe575-78a7-49d2-a857-aa6b101a4fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000  13487 180807 ./output/otest.crf\n"
     ]
    }
   ],
   "source": [
    "!wc ./output/otest.crf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe08557-c9b1-4fd0-b12b-15476185d7ed",
   "metadata": {},
   "source": [
    "## Word Segmentation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a182031d-637b-4b6c-9df4-2a0031fcea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 03:19:54.974776: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-26 03:19:54.975704: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 03:19:54.996012: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 03:19:54.996267: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-26 03:19:55.378127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-26 03:19:55.866804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-26 03:19:55.885154: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "\n",
      "real\t0m25.262s\n",
      "user\t0m26.509s\n",
      "sys\t0m3.656s\n"
     ]
    }
   ],
   "source": [
    "!time python ./baseline_segmenter.py --input ./data/otest.1k.word.input --output ./output/otest.lstm -m lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7cd3081-a39a-4092-9ca8-5927a7fa0eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "တစ် ကိုက် ကို ဝမ် ခု နှစ် ထောင် ပါ ။\n",
      "မနှစ် က သူ ကျွန်မ ကို သင်ပေး တယ် ။\n",
      "ကျွန်တော့် ခုံ သွား ရှာ မလို့ ။\n",
      "အတန်း စ တာ ကြာ ပြီ လား ။\n",
      "ဆေး နည်းနည်း စား လိုက် ၊ သုံး လေး ရက် လောက် အနားယူ လိုက် ရင် ပျောက် သွား မှာ ပါ ။\n",
      "အေးချမ်း မှု နဲ့ စည်းကမ်း ကို တည်မြဲ အောင် ထိန်းသိမ်း သည် ။\n",
      "ဇွန်း ကို လိုအပ် တယ် ။\n",
      "ဘွဲ့ ရ ရင် ဘာ လုပ် မ လို့ လဲ ။\n",
      "ကျွန်တော် ချောင်းဆိုး ခြင်း အတွက် တစ် ခု ခု လို ချင် တယ် ။\n",
      "အသီးအနှံ တို့ မှ လွဲ လျှင် လူ တို့ ၏ အဓိက အစားအစာ မှာ ငါး ဖြစ် သည် ။\n"
     ]
    }
   ],
   "source": [
    "!head ./output/otest.lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "929e76c6-b1c9-4f09-b5bb-04321632126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "အိုးခွက် ပန်းကန် တွေ သိပ် မ ရှိ လို့ ထမင်း ဟင်း ချက် ရ တာ အဆင်မပြေ ဘူး ။\n",
      "စိတ်ဝင်စား ဖို့ ကောင်း တယ် ။\n",
      "ဒီ ဆေး ကို တဝက် စီ ခွဲ ပေး ပါ ။\n",
      "ရောင်း ကောင်း လား ။\n",
      "ဆရာ ဒီ သွား က ခဏခဏ နာ နေ တယ် ။\n",
      "အခု ဘာ လုပ် နေ လဲ ။\n",
      "ဇူလိုင် ၁၄ ရက် မှာ ဘန်ကောက် ကို သွား မယ့် US 123 မှာ ပါ ။ ဟုတ် လား ။\n",
      "ကား မှ ကား ဘီး ကို ဖြုတ် လိုက် သည် ။\n",
      "ကျွန်တော် သိ ပါရစေ ။\n",
      "ဘူတာရုံ က အလွန်တရာ ပြည့်ကျပ် နေ သည် ။\n"
     ]
    }
   ],
   "source": [
    "!tail ./output/otest.lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d95356-7e39-46cb-ad59-7be9cffd9dde",
   "metadata": {},
   "source": [
    "## Evaluation for myWord\n",
    "\n",
    "ဒီနေရာမှာ evaluate.py နဲ့ eval_segmentation.py ပရိုဂရမ်နှစ်မျိုးလုံးသုံးပြီး evaluation လုပ်ပါမယ်။  \n",
    "evaluate.py ကတော့ 2006 ကတည်းက Yue Zhang (Computing lab, University of Oxford.) က ရေးခဲ့ပြီး word segmentation evaluation အတွက်သုံးခဲ့တဲ့ ပရိုဂရမ်ပါ။  \n",
    "သူကို run တဲ့အခါမှာ hyp ဖိုင်ကို အရင် ပေးရပါတယ်။  ပြီးတော့ python2.7 နဲ့ run ပါ။  \n",
    "\n",
    "myWord အတွက် အရင်ဆုံး evaluation လုပ်ပါမယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eddc1a4d-4deb-4b9d-ae25-d6f65a0c6965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag precision: 0.856264252643\n"
     ]
    }
   ],
   "source": [
    "!python2.7 ./evaluate.py ./output/otest.myword ./data/otest.1k.word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a3972fd-12c5-4532-a398-f1eddb60266b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Segmentation Evaluation Results\n",
      "============================================================\n",
      "Metric                         Score\n",
      "------------------------------------------------------------\n",
      "Word Precision                0.8489\n",
      "Word Recall                   0.9121\n",
      "Word F1-score                 0.8793\n",
      "------------------------------------------------------------\n",
      "Boundary Precision            0.4769\n",
      "Boundary Recall               0.5124\n",
      "Boundary F1-score             0.4940\n",
      "------------------------------------------------------------\n",
      "Vocab Precision               0.8911\n",
      "Vocab Recall                  0.7645\n",
      "Vocab F1-score                0.8230\n",
      "============================================================\n",
      "\n",
      "Additional Statistics:\n",
      "Reference words: 13468\n",
      "Hypothesis words: 14471\n",
      "Correct words: 12284\n",
      "Reference vocabulary size: 2709\n",
      "Hypothesis vocabulary size: 2324\n",
      "Common vocabulary: 2071\n",
      "\n",
      "Top Segmentation Errors Analysis\n",
      "============================================================\n",
      "Total errors: 6568\n",
      "\n",
      "Most Frequent Over-Segmentation Errors (System split where it shouldn't):\n",
      "\n",
      "Most Frequent Under-Segmentation Errors (System joined what should be separate):\n",
      "\n",
      "Most Frequent Complex Boundary Errors:\n",
      "   55 × REF: '။' → HYP: 'သည်'\n",
      "   53 × REF: '။' → HYP: 'တယ်'\n",
      "   37 × REF: '။' → HYP: 'လား'\n",
      "   33 × REF: '။' → HYP: 'ပါ'\n",
      "   27 × REF: '။' → HYP: 'လဲ'\n",
      "   22 × REF: 'မလား' → HYP: 'မ'\n",
      "   22 × REF: '။' → HYP: 'မ'\n",
      "   21 × REF: 'တယ်' → HYP: 'ပါ'\n",
      "   20 × REF: 'ခုနှစ်' → HYP: 'ခု'\n",
      "   14 × REF: 'မလဲ' → HYP: 'မ'\n"
     ]
    }
   ],
   "source": [
    "!python ./eval_segmentation.py -r ./data/otest.1k.word -H ./output/otest.myword --top-k 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750141c7-aa7d-4538-a010-54431479b478",
   "metadata": {},
   "source": [
    "## Evaluation for CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bc7a7a0-d7bd-48d5-b0ea-ac251b886916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag precision: 0.72239934752\n"
     ]
    }
   ],
   "source": [
    "!python2.7 ./evaluate.py ./output/otest.crf ./data/otest.1k.word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57247dbe-34d6-474a-ae82-9a142e0b24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Segmentation Evaluation Results\n",
      "============================================================\n",
      "Metric                         Score\n",
      "------------------------------------------------------------\n",
      "Word Precision                0.7019\n",
      "Word Recall                   0.7029\n",
      "Word F1-score                 0.7024\n",
      "------------------------------------------------------------\n",
      "Boundary Precision            0.3210\n",
      "Boundary Recall               0.3215\n",
      "Boundary F1-score             0.3213\n",
      "------------------------------------------------------------\n",
      "Vocab Precision               0.4646\n",
      "Vocab Recall                  0.5482\n",
      "Vocab F1-score                0.5030\n",
      "============================================================\n",
      "\n",
      "Additional Statistics:\n",
      "Reference words: 13468\n",
      "Hypothesis words: 13487\n",
      "Correct words: 9467\n",
      "Reference vocabulary size: 2709\n",
      "Hypothesis vocabulary size: 3196\n",
      "Common vocabulary: 1485\n",
      "\n",
      "Top Segmentation Errors Analysis\n",
      "============================================================\n",
      "Total errors: 8545\n",
      "\n",
      "Most Frequent Over-Segmentation Errors (System split where it shouldn't):\n",
      "\n",
      "Most Frequent Under-Segmentation Errors (System joined what should be separate):\n",
      "\n",
      "Most Frequent Complex Boundary Errors:\n",
      "   40 × REF: 'ပါ' → HYP: '။'\n",
      "   37 × REF: '။' → HYP: 'တယ်'\n",
      "   36 × REF: 'တယ်' → HYP: '။'\n",
      "   32 × REF: 'သည်' → HYP: '။'\n",
      "   30 × REF: '။' → HYP: 'သည်'\n",
      "   21 × REF: 'လဲ' → HYP: '။'\n",
      "   15 × REF: '။' → HYP: 'ပါ'\n",
      "   13 × REF: 'လား' → HYP: '။'\n",
      "   10 × REF: '။' → HYP: 'လား'\n",
      "   10 × REF: '။' → HYP: 'လဲ'\n"
     ]
    }
   ],
   "source": [
    "!python ./eval_segmentation.py -r ./data/otest.1k.word -H ./output/otest.crf --top-k 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9bbe0-8f5a-4b78-9cd6-c62525d6933f",
   "metadata": {},
   "source": [
    "## Evaluation for LSTM\n",
    "\n",
    "LSTM approach မော်ဒယ်နဲ့ ဖြတ်ပြီးရလာတဲ့ မြန်မာစာ စာကြောင်းတွေကိုလည်း evaluation လုပ်ကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e1748ae-d599-4c9b-80fb-8876fbc6a6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag precision: 0.906617326948\n"
     ]
    }
   ],
   "source": [
    "!python2.7 ./evaluate.py ./output/otest.lstm ./data/otest.1k.word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d68eec7-4a99-48ee-bf39-2ebf6681fbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Segmentation Evaluation Results\n",
      "============================================================\n",
      "Metric                         Score\n",
      "------------------------------------------------------------\n",
      "Word Precision                0.9055\n",
      "Word Recall                   0.9266\n",
      "Word F1-score                 0.9159\n",
      "------------------------------------------------------------\n",
      "Boundary Precision            0.6025\n",
      "Boundary Recall               0.6165\n",
      "Boundary F1-score             0.6094\n",
      "------------------------------------------------------------\n",
      "Vocab Precision               0.8091\n",
      "Vocab Recall                  0.8169\n",
      "Vocab F1-score                0.8130\n",
      "============================================================\n",
      "\n",
      "Additional Statistics:\n",
      "Reference words: 13468\n",
      "Hypothesis words: 13782\n",
      "Correct words: 12479\n",
      "Reference vocabulary size: 2709\n",
      "Hypothesis vocabulary size: 2735\n",
      "Common vocabulary: 2213\n",
      "\n",
      "Top Segmentation Errors Analysis\n",
      "============================================================\n",
      "Total errors: 5042\n",
      "\n",
      "Most Frequent Over-Segmentation Errors (System split where it shouldn't):\n",
      "\n",
      "Most Frequent Under-Segmentation Errors (System joined what should be separate):\n",
      "\n",
      "Most Frequent Complex Boundary Errors:\n",
      "   46 × REF: '။' → HYP: 'တယ်'\n",
      "   38 × REF: '။' → HYP: 'သည်'\n",
      "   21 × REF: 'သည်' → HYP: '။'\n",
      "   14 × REF: 'တယ်' → HYP: '။'\n",
      "   13 × REF: '။' → HYP: 'မယ်'\n",
      "   13 × REF: 'တယ်' → HYP: 'ပါ'\n",
      "   13 × REF: 'ပါ' → HYP: '။'\n",
      "   12 × REF: '။' → HYP: 'လား'\n",
      "   12 × REF: 'သည်' → HYP: 'ဖြစ်'\n",
      "   12 × REF: '။' → HYP: 'ပါ'\n"
     ]
    }
   ],
   "source": [
    "!python ./eval_segmentation.py -r ./data/otest.1k.word -H ./output/otest.lstm --top-k 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b7076-a2a8-4ff1-9322-bfee641bcf62",
   "metadata": {},
   "source": [
    "## Preparing a Big Test Data  \n",
    "\n",
    "ဒီတစ်ခါတော့ word segmenter တွေရဲ့ speed ကိုပါ တိုင်းတာချင်တဲ့အတွက် စာကြောင်းရေ 10K (တစ်သောင်း) ရှိတဲ့ test data ကို ဖန်တီးမယ်။  \n",
    "ပြီးတော့ မြန်မာ punctuation တွေလည်း ဖြုတ်ထားခဲ့ပြီး စမ်းမယ်။  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47690484-aa80-4676-8ff5-10bc347744c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/myTokenizer\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3c91900-dd24-4f16-b10d-ace5ad8f802c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-26 16:46:28--  https://raw.githubusercontent.com/ye-kyaw-thu/myPOS/refs/heads/master/corpus-ver-3.0/corpus/mypos-ver.3.0.shuf.notag.nopunc.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7303817 (7.0M) [application/octet-stream]\n",
      "Saving to: ‘mypos-ver.3.0.shuf.notag.nopunc.txt’\n",
      "\n",
      "mypos-ver.3.0.shuf. 100%[===================>]   6.96M  25.9MB/s    in 0.3s    \n",
      "\n",
      "2025-07-26 16:46:29 (25.9 MB/s) - ‘mypos-ver.3.0.shuf.notag.nopunc.txt’ saved [7303817/7303817]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ye-kyaw-thu/myPOS/refs/heads/master/corpus-ver-3.0/corpus/mypos-ver.3.0.shuf.notag.nopunc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e9ba4-8db8-4fd6-9fd6-ca5efec64b27",
   "metadata": {},
   "source": [
    "**Download လုပ်ယူထားတဲ့ ဖိုင်ထဲက ထိပ်ဆုံးပိုင်းစာကြောင်း တစ်သောင်းကို ဆွဲထုတ်ယူပြီး 10k_test.txt ဖိုင်အဖြစ် သိမ်းခဲ့တယ်။**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25e63e93-e411-446b-b744-710986081b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10000 ./mypos-ver.3.0.shuf.notag.nopunc.txt > 10k_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3aae9083-5084-4196-ab2e-a22619d774f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./10k_test.txt ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0559517-b017-4122-800b-d006457c4206",
   "metadata": {},
   "source": [
    "ဒီဖိုင်မှာက POS tag တွေက ဖြုတ်ထားပြီးသားပါ။ အဲဒါကြောင့် POS tag ဖြုတ်ရတဲ့ အလုပ်တော့ လုပ်ဖို့ မလိုအပ်ပါဘူး။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4d541ab-e564-41aa-8ccc-f2befab8d8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "၁၉၆၂ ခုနှစ် ခန့်မှန်း သန်းခေါင်စာရင်း အရ လူဦးရေ ၁၁၅၉၃၁ ယောက် ရှိ သည်\n",
      "လူ တိုင်း တွင် သင့်မြတ် လျော်ကန် စွာ ကန့်သတ် ထား သည့် အလုပ် လုပ် ချိန် အပြင် လစာ နှင့်တကွ အခါ ကာလ အားလျော်စွာ သတ်မှတ် ထား သည့် အလုပ် အားလပ်ရက် များ ပါဝင် သည့် အနားယူခွင့် နှင့် အားလပ်ခွင့် ခံစားပိုင်ခွင့် ရှိ သည်\n",
      "ဤ နည်း ကို စစ်ယူ သော နည်း ဟု ခေါ် သည်\n",
      "စာပြန်ပွဲ ဆို တာ က အာဂုံဆောင် အလွတ်ကျက် ထား တဲ့ ပိဋကတ်သုံးပုံ စာပေ တွေ ကို စာစစ် သံဃာတော်ကြီး တွေ ရဲ့ ရှေ့ မှာ အလွတ် ပြန် ပြီး ရွတ်ပြ ရ တာ ပေါ့\n",
      "ဒီ မှာ ကျွန်တော့် သက်သေခံကတ် ပါ\n",
      "၂ဝ ရာစု မြန်မာ့ သမိုင်း သန်းဝင်းလှိုင် ၂ဝဝ၉ ခု မေ လ ကံကော်ဝတ်ရည် စာပေ\n",
      "ကျွန်တော် မျက်မှန် တစ် လက် လုပ် ချင် ပါ တယ်\n",
      "ကျွန်တော် တို့ က ဒီ အမှု ရဲ့ ကြံရာပါ ကို ဖမ်းမိ ဖို့ ကြိုးစား ခဲ့ တယ်\n",
      "ကလေး မီးဖွား ဖို့ ခန့်မှန်း ရက် က ဘယ်တော့ ပါ လဲ\n",
      "အရိုးရှင်းဆုံး ကာဗိုဟိုက်ဒရိတ် မှာ ဂလူးကို့စ် ဂလက်တို့စ် ဖရပ်တို့စ် စသည့် မိုနိုဆက်ကရိုက် များ ဖြစ် သည်\n"
     ]
    }
   ],
   "source": [
    "!head ./data/10k_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331e28b-10af-400b-a0c7-6108b1308087",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "myTokenize ကို input လုပ်တဲ့အခါမှာ space တွေအကုန် ဖြုတ်ပေးထားရတာမို့ smart_space_remover.py ကို သုံးမယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "924d8121-288e-4aae-b82c-544e13d32f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/myTokenizer/data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94d3b1f7-7fe1-40a0-8393-6a60af624663",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./smart_space_remover.py --input ./10k_test.txt --output ./10k_test.input --mode my_not_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd41ee-9d82-4293-87ff-264885179e17",
   "metadata": {},
   "source": [
    "Space ဖြုတ်ပြီး word segmenter ကို pass လုပ်မယ့် test input ဖိုင်ကို ကြည့်ကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09aeb0fa-7fc0-4b78-a212-15ba9e3ac49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "၁၉၆၂ ခုနှစ်ခန့်မှန်းသန်းခေါင်စာရင်းအရလူဦးရေ ၁၁၅၉၃၁ ယောက်ရှိသည်\n",
      "လူတိုင်းတွင်သင့်မြတ်လျော်ကန်စွာကန့်သတ်ထားသည့်အလုပ်လုပ်ချိန်အပြင်လစာနှင့်တကွအခါကာလအားလျော်စွာသတ်မှတ်ထားသည့်အလုပ်အားလပ်ရက်များပါဝင်သည့်အနားယူခွင့်နှင့်အားလပ်ခွင့်ခံစားပိုင်ခွင့်ရှိသည်\n",
      "ဤနည်းကိုစစ်ယူသောနည်းဟုခေါ်သည်\n",
      "စာပြန်ပွဲဆိုတာကအာဂုံဆောင်အလွတ်ကျက်ထားတဲ့ပိဋကတ်သုံးပုံစာပေတွေကိုစာစစ်သံဃာတော်ကြီးတွေရဲ့ရှေ့မှာအလွတ်ပြန်ပြီးရွတ်ပြရတာပေါ့\n",
      "ဒီမှာကျွန်တော့်သက်သေခံကတ်ပါ\n",
      "၂ဝရာစုမြန်မာ့သမိုင်းသန်းဝင်းလှိုင် ၂ဝဝ၉ ခုမေလကံကော်ဝတ်ရည်စာပေ\n",
      "ကျွန်တော်မျက်မှန်တစ်လက်လုပ်ချင်ပါတယ်\n",
      "ကျွန်တော်တို့ကဒီအမှုရဲ့ကြံရာပါကိုဖမ်းမိဖို့ကြိုးစားခဲ့တယ်\n",
      "ကလေးမီးဖွားဖို့ခန့်မှန်းရက်ကဘယ်တော့ပါလဲ\n",
      "အရိုးရှင်းဆုံးကာဗိုဟိုက်ဒရိတ်မှာဂလူးကို့စ်ဂလက်တို့စ်ဖရပ်တို့စ်စသည့်မိုနိုဆက်ကရိုက်များဖြစ်သည်\n"
     ]
    }
   ],
   "source": [
    "!head ./10k_test.input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5173e1dc-c36f-45cb-adb0-f795b9e352bd",
   "metadata": {},
   "source": [
    "## Word Segmentation with myWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "525a0219-ee04-448c-82c7-ca218bc5a0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/myTokenizer\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7cebd0b5-55d6-406c-b61d-4c817fdd2154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 17:25:57.185836: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-26 17:25:57.186801: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 17:25:57.207083: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 17:25:57.207334: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-26 17:25:57.607601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "real\t89m50.495s\n",
      "user\t81m56.812s\n",
      "sys\t7m55.380s\n"
     ]
    }
   ],
   "source": [
    "!time python ./baseline_segmenter.py --input ./data/10k_test.input --output ./output/10k_test.myword -m myword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5e05e23a-a7f1-4a8d-8393-88a4d2951b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10000  128206 1696728 ./output/10k_test.myword\n"
     ]
    }
   ],
   "source": [
    "!wc ./output/10k_test.myword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d439253c-d495-4096-9a0e-d4f4ed05d402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "၁၉၆၂ ခု နှစ် ခန့်မှန်း သန်းခေါင်စာရင်း အရ လူ ဦး ရေ ၁၁၅၉၃၁ ယောက် ရှိ သည်\n",
      "လူ တိုင်း တွင် သင့်မြတ် လျော်ကန် စွာ ကန့်သတ် ထား သည့် အလုပ် လုပ် ချိန် အပြင် လစာ နှင့်တကွ အခါ ကာလ အားလျော်စွာ သတ်မှတ် ထား သည့် အလုပ် အားလပ်ရက် များ ပါ ဝင် သည့် အနားယူ ခွင့် နှင့် အားလပ် ခွင့် ခံစားပိုင်ခွင့် ရှိ သည်\n",
      "ဤ နည်း ကို စစ် ယူ သော နည်း ဟု ခေါ် သည်\n",
      "စာ ပြန် ပွဲ ဆို တာ က အာဂုံဆောင် အလွတ်ကျက် ထား တဲ့ ပိဋကတ်သုံးပုံ စာပေ တွေ ကို စာ စစ် သံဃာတော် ကြီး တွေ ရဲ့ ရှေ့ မှာ အလွတ် ပြန် ပြီး ရွတ် ပြ ရ တာ ပေါ့\n",
      "ဒီ မှာ ကျွန်တော့် သက်သေခံ ကတ် ပါ\n",
      "၂ဝ ရာ စု မြန်မာ့ သမိုင်း သန်းဝင်းလှိုင် ၂ဝဝ၉ ခု မေ လ ကံကော်ဝတ်ရည် စာပေ\n",
      "ကျွန်တော် မျက်မှန် တစ် လက် လုပ် ချင် ပါ တယ်\n",
      "ကျွန်တော် တို့ က ဒီ အမှု ရဲ့ ကြံ ရာ ပါ ကို ဖမ်း မိ ဖို့ ကြိုးစား ခဲ့ တယ်\n",
      "က လေး မီးဖွား ဖို့ ခန့်မှန်း ရက် က ဘယ် တော့ ပါ လဲ\n",
      "အရိုးရှင်းဆုံး ကာဗိုဟိုက်ဒရိတ် မှာ ဂလူးကို့စ် ဂလက်တို့စ် ဖရပ်တို့စ် စ သည့် မိုနိုဆက်ကရိုက် များ ဖြစ် သည်\n"
     ]
    }
   ],
   "source": [
    "!head ./output/10k_test.myword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3777b2c3-c69d-4710-b84b-1abb6a3a8813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ကျွန်တော် စိန် နဲ့ တစ် ခု လို ချင် ပါ တယ်\n",
      "ကြိုး ကြာ\n",
      "သူ အဘိဓာန်စာလုံး ရှာ တတ် ပါ တယ်\n",
      "ဪ ကျွန်တော် မျက်စိလည် နေ ပြီ ထင် တယ် ဒီ နေ ရာ က နေ ဘူတာရုံ ကို ဘယ်လို သွား ရ မ လဲ\n",
      "လော့အိန်ဂျလိစ် က နေ တိုကျို ကို လေယာဉ် ခ ဘယ်လောက် လဲ\n",
      "ပထမဆုံး အဆင်ပြေ မယ့် လေယာဉ် လို ချင် ပါ တယ်\n",
      "ရေ ထွက် ပစ္စည်း များ ထုတ် လုပ် မှု တွင် လည်း တန် ချိန် ၄၁.၂၂၄ သန်း ထိ ၁၉၇၈ ခု နှစ် ထက် ၈.၈ ဆ တိုးတက် ထုတ် လုပ် နိုင် ခဲ့ သည်\n",
      "ဟုတ် တယ် ဟင်းသီးဟင်းရွက် သွား ဝယ် ရ အောင်\n",
      "ဆရာ ကြီး က ၄ နာရီ လောက် ဆို အလုပ် နည်းနည်း ရှင်း ပြီ ၅ နာရီ မှ ရုံး ဆင်း မှာ ဆို တော့ တစ် နာရီ လောက် တော့ တွေ့ ချိန် ရ မှာ ပဲ\n",
      "ဝင် ခွင့် ဈေးနှုန်း ရော ပါ လား\n"
     ]
    }
   ],
   "source": [
    "!tail ./output/10k_test.myword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860bf526-b79d-4668-bae0-e6a4d953aab3",
   "metadata": {},
   "source": [
    "## Word Segmentation with CRF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cfcd783f-8604-4d7f-9c7a-15384bf55ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 19:40:39.289427: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-26 19:40:39.290426: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 19:40:39.310621: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 19:40:39.310889: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-26 19:40:39.692507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "real\t0m2.641s\n",
      "user\t0m3.190s\n",
      "sys\t0m1.993s\n"
     ]
    }
   ],
   "source": [
    "!time python ./baseline_segmenter.py --input ./data/10k_test.input --output ./output/10k_test.crf -m crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "257d796e-fe00-452f-b009-3bd215bcd401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "၁၉၆၂ ခုနှစ် ခန့်မှန်းသန်းခေါင်စာရင်း အ ရ လူဦးရေ ၁၁၅၉၃၁ ယောက်ရှိ သည်\n",
      "လူ တိုင်း တွင် သင့်မြတ်လျော်ကန် စွာ ကန့်သတ်ထား သည့် အလုပ်လုပ်ချိန် အပြင်လစာ နှင့်တကွ အ ခါ ကာလ အားလျော် စွာ သတ် မှတ်ထား သည့် အလုပ် အားလပ် ရက် များ ပါဝင် သည့် အနားယူ ခွင့် နှင့် အားလပ်ခွင့် ခံစား ပိုင်ခွင့် ရှိ သည်\n",
      "ဤနည်း ကို စစ်ယူ သော နည်း ဟု ခေါ် သည်\n",
      "စာပြန်ပွဲ ဆို တာ က အာဂုံ ဆောင် အ လွတ်ကျက်ထား တဲ့ ပိဋကတ်သုံးပုံစာ ပေ တွေ ကို စာစစ်သံဃာတော်ကြီး တွေ ရဲ့ ရှေ့ မှာ အ လွတ်ပြန် ပြီး ရွတ်ပြ ရ တာ ပေါ့\n",
      "ဒီ မှာ ကျွန်တော့် သက် သေခံ ကတ် ပါ\n",
      "၂ဝရာစု မြန်မာ့ သမိုင်းသန်းဝင်း လှိုင် ၂ဝဝ၉ ခုမေ လ ကံကော်ဝတ်ရည်စာပေ\n",
      "ကျွန်တော် မျက် မှန် တစ် လက်လုပ်ချင် ပါ တယ်\n",
      "ကျွန်တော် တို့ က ဒီ အမှု ရဲ့ ကြံ ရာ ပါ ကို ဖမ်း မိ ဖို့ကြိုးစား ခဲ့ တယ်\n",
      "ကလေး မီးဖွား ဖို့ခန့် မှန်းရက် က ဘယ်တော့ ပါ လဲ\n",
      "အရိုးရှင်းဆုံး ကာ ဗိုဟိုက်ဒရိတ်မှာ ဂ လူး ကို့စ်ဂလက်တို့စ်ဖ ရပ်တို့စ် စသည့် မို နို ဆက်ကရိုက် များ ဖြစ် သည်\n"
     ]
    }
   ],
   "source": [
    "!head ./output/10k_test.crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "919272f3-c8be-42b7-9697-fe7b4c09d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ကျွန်တော်စိန် နဲ့ တစ် ခု လို ချင် ပါ တယ်\n",
      "ကြိုးကြာ\n",
      "သူ အ ဘိဓာန်စာလုံး ရှာ တတ် ပါ တယ်\n",
      "ဪ ကျွန်တော် မျက်စိ လည် နေ ပြီ ထင် တယ် ဒီ နေရာ က နေ ဘူတာ ရုံ ကို ဘယ်လို သွား ရ မလဲ\n",
      "လော့ အိန်ဂျလိစ် က နေ တိုကျိုကို လေယာဉ်ခ ဘယ်လောက် လဲ\n",
      "ပထမဆုံး အ ဆင်ပြေ မယ့် လေယာဉ် လို ချင် ပါ တယ်\n",
      "ရေထွက် ပစ္စည်း များ ထုတ်လုပ် မှု တွင် လည်းတန်ချိန် ၄၁.၂၂၄ သန်းထိ ၁၉၇၈ ခုနှစ် ထက်၈.၈ ဆ တိုးတက်ထုတ်လုပ် နိုင် ခဲ့ သည်\n",
      "ဟုတ် တယ် ဟင်းသီးဟင်းရွက် သွား ဝယ် ရ အောင်\n",
      "ဆရာကြီး က၄ နာရီ လောက်ဆို အလုပ် နည်းနည်း ရှင်း ပြီ ၅ နာရီ မှ ရုံးဆင်း မှာ ဆို တော့ တစ် နာရီ လောက်တော့ တွေ့ချိန် ရ မှာ ပဲ\n",
      "ဝင်ခွင့် ဈေးနှုန်း ရော ပါ လား\n"
     ]
    }
   ],
   "source": [
    "!tail ./output/10k_test.crf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fabe4f0-9e41-44cf-a484-55ddee4a9a80",
   "metadata": {},
   "source": [
    "## Word Segmentation with LSTM\n",
    "\n",
    "စာကြောင်းရေ တစ်သောင်း ရှိတဲ့ test ဖိုင်ကို ဒီတခါတော့ LSTM မော်ဒယ်နဲ့ word segmentation လုပ်မယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9173cf47-4d46-4083-b69f-6e9fe44548ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 19:42:09.683039: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-26 19:42:09.683983: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 19:42:09.704895: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-26 19:42:09.705143: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-26 19:42:10.087859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-26 19:42:10.583264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-26 19:42:10.604644: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "\n",
      "real\t3m39.747s\n",
      "user\t3m46.339s\n",
      "sys\t0m17.206s\n"
     ]
    }
   ],
   "source": [
    "!time python ./baseline_segmenter.py --input ./data/10k_test.input --output ./output/10k_test.lstm -m lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "336a38b2-a7d7-49fc-bf0b-1ccf415961f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "၁၉၆၂ ခုနှစ် ခန့်မှန်း သန်းခေါင်စာရင်း အရ လူဦးရေ ၁၁၅၉၃၁ ယောက် ရှိ သည်\n",
      "လူ တိုင်း တွင် သင့်မြတ် လျော်ကန် စွာ ကန့်သတ် ထား သည့် အလုပ် လုပ် ချိန် အပြင် လစာ နှင့်တကွ အခါ ကာလ အားလျော် စွာ သတ်မှတ် ထား သည့် အလုပ်အားလပ်ရက် များ ပါဝင် သည့် အနားယူ ခွင့် နှင့် အားလပ်ခွင့် ခံစား ပိုင် ခွင့် ရှိ သည်\n",
      "ဤ နည်း ကို စစ်ယူ သော နည်း ဟု ခေါ် သည်\n",
      "စာပြန် ပွဲ ဆို တာ က အာဂုံ ဆောင် အလွတ်ကျက် ထား တဲ့ ပိဋကတ် သုံး ပုံ စာပေ တွေ ကို စာစစ် သံဃာတော် ကြီး တွေ ရဲ့ ရှေ့ မှာ အလွတ် ပြန် ပြီး ရွတ်ပြ ရ တာ ပေါ့\n",
      "ဒီ မှာ ကျွန်တော့် သက် သေ ခံကတ် ပါ\n",
      "၂ဝ ရာစု မြန်မာ့ သမိုင်း သန်းဝင်းလှိုင် ၂ဝဝ ၉ ခုမေ လ ကံ ကော်ဝတ်ရည် စာပေ\n",
      "ကျွန်တော် မျက်မှန် တစ် လက် လုပ် ချင် ပါ တယ်\n",
      "ကျွန်တော် တို့ က ဒီ အမှု ရဲ့ ကြံ ရာ ပါ ကို ဖမ်း မိ ဖို့ ကြိုးစား ခဲ့ တယ်\n",
      "ကလေး မီးဖွား ဖို့ ခန့်မှန်း ရက် က ဘယ်တော့ ပါ လဲ\n",
      "အရိုးရှင်းဆုံး ကာဗိုဟိုက်ဒရိတ် မှာ ဂလူးကို့စ် ဂလက်တို့စ်ဖရပ် တို့စ် စသည့် မိုနိုဆက် က ရိုက် များ ဖြစ် သည်\n"
     ]
    }
   ],
   "source": [
    "!head ./output/10k_test.lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d160c628-8067-4324-ad35-eda3d962d576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ကျွန်တော်စိန် နဲ့ တစ် ခု လို ချင် ပါ တယ်\n",
      "ကြိုး ကြာ\n",
      "သူ အဘိဓာန် စာလုံး ရှာ တတ် ပါ တယ်\n",
      "ဪ ကျွန်တော် မျက်စိ လည် နေ ပြီ ထင် တယ် ဒီ နေရာ က နေ ဘူတာရုံ ကို ဘယ်လို သွား ရ မလဲ\n",
      "လော့အိန်ဂျလိစ် က နေ တိုကျို ကို လေယာဉ် ခ ဘယ်လောက် လဲ\n",
      "ပထမဆုံး အဆင်ပြေ မယ့် လေယာဉ် လို ချင် ပါ တယ်\n",
      "ရေထွက် ပစ္စည်း များ ထုတ်လုပ် မှု တွင် လည်း တန်ချိန် ၄၁.၂၂၄ သန်း ထိ ၁၉၇၈ ခုနှစ် ထက် ၈.၈ ဆ တိုးတက် ထုတ်လုပ် နိုင် ခဲ့ သည်\n",
      "ဟုတ် တယ် ဟင်းသီးဟင်းရွက် သွား ဝယ် ရ အောင်\n",
      "ဆရာကြီး က ၄ နာရီ လောက် ဆို အလုပ် နည်းနည်း ရှင်း ပြီ ၅ နာရီ မှ ရုံး ဆင်း မှာ ဆို တော့ တစ် နာရီ လောက် တော့ တွေ့ ချိန် ရ မှာ ပဲ\n",
      "ဝင်ခွင့် ဈေးနှုန်း ရော ပါ လား\n"
     ]
    }
   ],
   "source": [
    "!tail ./output/10k_test.lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961ff52-f240-4858-a440-5bb6275c4748",
   "metadata": {},
   "source": [
    "## Evaluation for Big Test Dataset\n",
    "\n",
    "ဒီ ဒေတာ က တကယ်ကတော့ closed-test လို့ ယူဆလို့ ရပါတယ်။  \n",
    "ထပ်မှာမယ်။ evaluate.py နဲ့ evaluation လုပ်တဲ့အခါမှာ hypothesis ဖိုင်ကို အရင်ပေးပါ။ ပြီးမှ reference (manually segmented file) ကို argument အနေနဲ့ ပေးပြီး run ပါ။  \n",
    "\n",
    "အရင်ဆုံး myWord နဲ့ ဖြတ်ပြီးထွက်လာတဲ့ output ကို evaluation လုပ်ပါမယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "08761da7-e21c-4316-9767-0044c1f4564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag precision: 0.834305726721\n"
     ]
    }
   ],
   "source": [
    "!python2.7 ./evaluate.py ./output/10k_test.myword ./data/10k_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d4c02-f882-45a9-921d-3f5345762aa7",
   "metadata": {},
   "source": [
    "ဆရာရေးထားတဲ့ eval_segmentation.py ကို run တဲ့အခါမှာတော့ -r, -H နဲ့ အစီအစဉ်က သတ်မှတ်လို့ ပါတယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a1f3134f-515d-4527-b483-a8f1ecdb62cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Segmentation Evaluation Results\n",
      "============================================================\n",
      "Metric                         Score\n",
      "------------------------------------------------------------\n",
      "Word Precision                0.8235\n",
      "Word Recall                   0.8958\n",
      "Word F1-score                 0.8581\n",
      "------------------------------------------------------------\n",
      "Boundary Precision            0.4695\n",
      "Boundary Recall               0.5107\n",
      "Boundary F1-score             0.4893\n",
      "------------------------------------------------------------\n",
      "Vocab Precision               0.8953\n",
      "Vocab Recall                  0.6456\n",
      "Vocab F1-score                0.7502\n",
      "============================================================\n",
      "\n",
      "Additional Statistics:\n",
      "Reference words: 117857\n",
      "Hypothesis words: 128206\n",
      "Correct words: 105579\n",
      "Reference vocabulary size: 10840\n",
      "Hypothesis vocabulary size: 7816\n",
      "Common vocabulary: 6998\n",
      "\n",
      "Top Segmentation Errors Analysis\n",
      "============================================================\n",
      "Total errors: 57570\n",
      "\n",
      "Most Frequent Over-Segmentation Errors (System split where it shouldn't):\n",
      "\n",
      "Most Frequent Under-Segmentation Errors (System joined what should be separate):\n",
      "\n",
      "Most Frequent Complex Boundary Errors:\n",
      "  176 × REF: 'မလား' → HYP: 'မ'\n",
      "  172 × REF: 'မလဲ' → HYP: 'မ'\n",
      "  170 × REF: 'တယ်' → HYP: 'ပါ'\n",
      "  148 × REF: 'နေရာ' → HYP: 'နေ'\n",
      "  128 × REF: 'ခုနှစ်' → HYP: 'ခု'\n",
      "   97 × REF: 'သည်' → HYP: 'ခဲ့'\n",
      "   95 × REF: 'သည်' → HYP: 'ဖြစ်'\n",
      "   88 × REF: 'ရထား' → HYP: 'ရ'\n",
      "   73 × REF: 'တယ်' → HYP: 'ခဲ့'\n",
      "   71 × REF: 'ကလေး' → HYP: 'က'\n",
      "   67 × REF: 'ကို' → HYP: 'များ'\n",
      "   62 × REF: 'ဘာသာ' → HYP: 'ဘာ'\n",
      "   62 × REF: 'သည်' → HYP: 'ရှိ'\n",
      "   62 × REF: 'သည်' → HYP: 'ကြ'\n",
      "   59 × REF: 'တယ်' → HYP: 'နေ'\n",
      "   59 × REF: 'သူမ' → HYP: 'သူ'\n",
      "   58 × REF: 'သည်' → HYP: 'များ'\n",
      "   58 × REF: 'ရ' → HYP: 'လို့'\n",
      "   57 × REF: 'ပါ' → HYP: 'ပေး'\n",
      "   55 × REF: 'မှာ' → HYP: 'ရာ'\n",
      "   55 × REF: 'ခု' → HYP: 'တစ်'\n",
      "   54 × REF: 'သည်' → HYP: 'ပါ'\n",
      "   53 × REF: 'ပါ' → HYP: 'မ'\n",
      "   50 × REF: 'ဘယ်သူ' → HYP: 'ဘယ်'\n",
      "   49 × REF: 'ပါ' → HYP: 'ရှိ'\n",
      "   48 × REF: 'တယ်' → HYP: 'ရှိ'\n",
      "   48 × REF: 'ဘူး' → HYP: 'မ'\n",
      "   47 × REF: 'ပါ' → HYP: 'ချင်'\n",
      "   47 × REF: 'သည်' → HYP: 'ရ'\n",
      "   44 × REF: 'တွင်' → HYP: 'နှစ်'\n"
     ]
    }
   ],
   "source": [
    "!python ./eval_segmentation.py -H ./output/10k_test.myword -r ./data/10k_test.txt --top-k 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb21fe-0fd8-490e-b7d2-20015d20950c",
   "metadata": {},
   "source": [
    "CRF နဲ့ ဖြတ်ထားတဲ့ ဖိုင်ကို evaluation လုပ်ကြည့်မယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "047e3168-1da0-4cdc-94c8-ca80c2790b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag precision: 0.704589402575\n"
     ]
    }
   ],
   "source": [
    "!python2.7 ./evaluate.py ./output/10k_test.crf ./data/10k_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7feae4ac-f27a-4a0e-8f80-a67b0e53b815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Segmentation Evaluation Results\n",
      "============================================================\n",
      "Metric                         Score\n",
      "------------------------------------------------------------\n",
      "Word Precision                0.6928\n",
      "Word Recall                   0.6961\n",
      "Word F1-score                 0.6944\n",
      "------------------------------------------------------------\n",
      "Boundary Precision            0.3186\n",
      "Boundary Recall               0.3201\n",
      "Boundary F1-score             0.3194\n",
      "------------------------------------------------------------\n",
      "Vocab Precision               0.3256\n",
      "Vocab Recall                  0.4674\n",
      "Vocab F1-score                0.3838\n",
      "============================================================\n",
      "\n",
      "Additional Statistics:\n",
      "Reference words: 117857\n",
      "Hypothesis words: 118425\n",
      "Correct words: 82042\n",
      "Reference vocabulary size: 10840\n",
      "Hypothesis vocabulary size: 15563\n",
      "Common vocabulary: 5067\n",
      "\n",
      "Top Segmentation Errors Analysis\n",
      "============================================================\n",
      "Total errors: 74443\n",
      "\n",
      "Most Frequent Over-Segmentation Errors (System split where it shouldn't):\n",
      "\n",
      "Most Frequent Under-Segmentation Errors (System joined what should be separate):\n",
      "\n",
      "Most Frequent Complex Boundary Errors:\n",
      "  152 × REF: 'ပါ' → HYP: 'တယ်'\n",
      "  114 × REF: 'တယ်' → HYP: 'ပါ'\n",
      "   74 × REF: 'အတွက်' → HYP: 'အ'\n",
      "   70 × REF: 'သည်' → HYP: 'ဖြစ်'\n",
      "   67 × REF: 'ဘယ်' → HYP: 'ဘယ်မှာ'\n",
      "   67 × REF: 'သည်' → HYP: 'ခဲ့'\n",
      "   63 × REF: 'လို့' → HYP: 'ရ'\n",
      "   60 × REF: 'ဘယ်' → HYP: 'ဘယ်အချိန်'\n",
      "   58 × REF: 'ဖြစ်' → HYP: 'သည်'\n",
      "   56 × REF: 'မှာ' → HYP: 'လဲ'\n",
      "   56 × REF: 'ကို' → HYP: 'အ'\n",
      "   56 × REF: 'ကို' → HYP: 'များ'\n",
      "   54 × REF: 'ခု' → HYP: 'တစ်'\n",
      "   54 × REF: 'တယ်' → HYP: 'ခဲ့'\n",
      "   53 × REF: 'နိုင်' → HYP: 'နိုင်မလား'\n",
      "   52 × REF: 'သည်' → HYP: 'များ'\n",
      "   50 × REF: 'ပါ' → HYP: 'ပေး'\n",
      "   49 × REF: 'ခဲ့' → HYP: 'သည်'\n",
      "   49 × REF: 'ချင်' → HYP: 'တယ်'\n",
      "   48 × REF: 'ကိစ္စ' → HYP: 'ကိစ္'\n",
      "   47 × REF: 'ရှိ' → HYP: 'ပါ'\n",
      "   47 × REF: 'များ' → HYP: 'ကို'\n",
      "   46 × REF: 'ရှိ' → HYP: 'မ'\n",
      "   44 × REF: 'ပေး' → HYP: 'ပါ'\n",
      "   43 × REF: 'ချင်' → HYP: 'ပါ'\n",
      "   43 × REF: 'ရ' → HYP: 'လို့'\n",
      "   41 × REF: 'ပါ' → HYP: 'ရှိ'\n",
      "   40 × REF: 'မနက်ဖြန်' → HYP: 'မနက်'\n",
      "   39 × REF: 'နေ' → HYP: 'တယ်'\n",
      "   39 × REF: 'ကြ' → HYP: 'သည်'\n"
     ]
    }
   ],
   "source": [
    "!python ./eval_segmentation.py -H ./output/10k_test.crf -r ./data/10k_test.txt --top-k 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9fbb1-20ac-4bcb-9b39-f3a3b18d70d4",
   "metadata": {},
   "source": [
    "Neural network approach ဖြစ်တဲ့ lstm နဲ့ ဖြတ်ထားတဲ့ output ဖိုင်ကိုလည်း evaluation လုပ်မယ်။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2cda50c-2c2f-4730-80d7-d9fa7b566e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag precision: 0.898953005049\n"
     ]
    }
   ],
   "source": [
    "!python2.7 ./evaluate.py ./output/10k_test.lstm ./data/10k_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "48fa4b01-b34d-47ee-a4ab-9b8946f694bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Segmentation Evaluation Results\n",
      "============================================================\n",
      "Metric                         Score\n",
      "------------------------------------------------------------\n",
      "Word Precision                0.8970\n",
      "Word Recall                   0.9210\n",
      "Word F1-score                 0.9088\n",
      "------------------------------------------------------------\n",
      "Boundary Precision            0.6119\n",
      "Boundary Recall               0.6283\n",
      "Boundary F1-score             0.6200\n",
      "------------------------------------------------------------\n",
      "Vocab Precision               0.7265\n",
      "Vocab Recall                  0.7093\n",
      "Vocab F1-score                0.7178\n",
      "============================================================\n",
      "\n",
      "Additional Statistics:\n",
      "Reference words: 117857\n",
      "Hypothesis words: 121013\n",
      "Correct words: 108546\n",
      "Reference vocabulary size: 10840\n",
      "Hypothesis vocabulary size: 10584\n",
      "Common vocabulary: 7689\n",
      "\n",
      "Top Segmentation Errors Analysis\n",
      "============================================================\n",
      "Total errors: 42500\n",
      "\n",
      "Most Frequent Over-Segmentation Errors (System split where it shouldn't):\n",
      "\n",
      "Most Frequent Under-Segmentation Errors (System joined what should be separate):\n",
      "\n",
      "Most Frequent Complex Boundary Errors:\n",
      "  133 × REF: 'တယ်' → HYP: 'ပါ'\n",
      "   91 × REF: 'သည်' → HYP: 'ဖြစ်'\n",
      "   73 × REF: 'တယ်' → HYP: 'ခဲ့'\n",
      "   73 × REF: 'သည်' → HYP: 'ခဲ့'\n",
      "   57 × REF: 'ဖြစ်' → HYP: 'သည်'\n",
      "   56 × REF: 'ပါ' → HYP: 'တယ်'\n",
      "   53 × REF: 'ပါ' → HYP: 'ပေး'\n",
      "   51 × REF: 'တယ်' → HYP: 'နေ'\n",
      "   47 × REF: 'များ' → HYP: 'ကို'\n",
      "   43 × REF: 'သည်' → HYP: 'ကြ'\n",
      "   41 × REF: 'ခဲ့' → HYP: 'သည်'\n",
      "   40 × REF: 'ပါ' → HYP: 'ရှိ'\n",
      "   39 × REF: 'ခု' → HYP: 'တစ်'\n",
      "   38 × REF: 'ကြ' → HYP: 'သည်'\n",
      "   38 × REF: 'တယ်' → HYP: 'ရှိ'\n",
      "   37 × REF: 'သည်' → HYP: 'ရှိ'\n",
      "   36 × REF: 'ရ' → HYP: 'လို့'\n",
      "   35 × REF: 'ရအောင်' → HYP: 'ရ'\n",
      "   34 × REF: 'ကို' → HYP: 'များ'\n",
      "   33 × REF: 'သည်' → HYP: 'များ'\n",
      "   33 × REF: 'မလား' → HYP: 'နိုင်'\n",
      "   32 × REF: 'သည်' → HYP: 'ရ'\n",
      "   30 × REF: 'သွား' → HYP: 'ကို'\n",
      "   30 × REF: 'သည်' → HYP: 'လေ'\n",
      "   30 × REF: 'ရှိ' → HYP: 'မ'\n",
      "   29 × REF: 'တယ်' → HYP: 'ရ'\n",
      "   29 × REF: 'ပါ' → HYP: 'ချင်'\n",
      "   29 × REF: 'သည်' → HYP: 'ပါ'\n",
      "   27 × REF: 'ပါ' → HYP: 'ရ'\n",
      "   26 × REF: 'ရှိ' → HYP: 'သည်'\n"
     ]
    }
   ],
   "source": [
    "!python ./eval_segmentation.py -H ./output/10k_test.lstm -r ./data/10k_test.txt --top-k 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4067a6ec-a0c8-4bba-b67c-23d32bf75ad9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "လက်ရှိအချိန်ထိ Lab မှာ သုံးခဲ့ကြတဲ့ word segmenter သုံးမျိုးကို open-test, closed-test လုပ်ကြည့်ခဲ့တယ်။ တနည်းအားဖြင့် စာကြောင်း အရေအတွက် စုစုပေါင်း တစ်ထောင်ရှိတဲ့ open-test ဒေတာနဲ့ စာကြောင်း အရေအတွက် တစ်သောင်း ရှိတဲ့ closed-test ဒေတာတွေနဲ့ baseline experiment အနေနဲ့ လုပ်ကြည့်ခဲ့ပါတယ်။ ရည်ရွယ်ချက်ကတော့ အသစ် develop လုပ်ထားတဲ့ dag_bimm_segmenter ရဲ့ word segmentation performance, runtime တွေကို နှိုင်းယှဉ်ကြည့်ချင်လို့ပါ။   \n",
    "\n",
    "ရလဒ်တွေကတော့ အောက်ပါအတိုင်းပါ။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f906ecc-ef07-4af0-9250-2571cad7de1a",
   "metadata": {},
   "source": [
    "## Word Metrics (Open-Test)\n",
    "\n",
    "| Model     | Precision | Recall  | F1-score |\n",
    "|-----------|-----------|---------|----------|\n",
    "| myWord    | 0.8489    | 0.9121  | 0.8793   |\n",
    "| CRF       | 0.7019    | 0.7029  | 0.7024   |\n",
    "| LSTM      | 0.9055    | 0.9266  | **0.9159** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59320b65-cea8-4310-9f56-a54867b5d129",
   "metadata": {},
   "source": [
    "## Boundary Metrics (Open-Test)\n",
    "\n",
    "| Model     | Precision | Recall  | F1-score |\n",
    "|-----------|-----------|---------|----------|\n",
    "| myWord    | 0.4769    | 0.5124  | 0.4940   |\n",
    "| CRF       | 0.3210    | 0.3215  | 0.3213   |\n",
    "| LSTM      | 0.6025    | 0.6165  | **0.6094** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b241b5-48b5-4708-941d-f8c778b92b04",
   "metadata": {},
   "source": [
    "## Vocab Metrics for Open-Test\n",
    "\n",
    "| Model     | Precision | Recall  | F1-score |\n",
    "|-----------|-----------|---------|----------|\n",
    "| myWord    | 0.8911    | 0.7645  | **0.8230** |\n",
    "| CRF       | 0.4646    | 0.5482  | 0.5030   |\n",
    "| LSTM      | 0.8091    | 0.8169  | 0.8130   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f53b6ab-0535-46f7-844e-71aead7ab05e",
   "metadata": {},
   "source": [
    "## Additional Evaluation Statistics (Open-Test)\n",
    "\n",
    "| **Metric**                     | **myWord** | **CRF** | **LSTM** |\n",
    "|--------------------------------|------------|---------|----------|\n",
    "| **Reference words**            | 13468      | 13468   | 13468    |\n",
    "| **Hypothesis words**           | 14471      | 13487   | 13782    |\n",
    "| **Correct words**              | 12284      | 9467    | 12479    |\n",
    "| **Reference vocab size**       | 2709       | 2709    | 2709     |\n",
    "| **Hypothesis vocab size**      | 2324       | 3196    | 2735     |\n",
    "| **Common vocabulary**          | 2071       | 1485    | 2213     |\n",
    "| **Total segmentation errors**  | 6568       | 8545    | 5042     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548d134-e610-42e7-b897-108b585b2df2",
   "metadata": {},
   "source": [
    "## Word Metrics (Closed Test)\n",
    "\n",
    "| Model     | Precision | Recall  | F1-score |\n",
    "|-----------|-----------|---------|----------|\n",
    "| myWord    | 0.8235    | 0.8958  | 0.8581   |\n",
    "| CRF       | 0.6928    | 0.6961  | 0.6944   |\n",
    "| LSTM      | 0.8970    | 0.9210  | **0.9088** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469bd054-2c4f-4500-b591-fa2ef9e31615",
   "metadata": {},
   "source": [
    "## Boundary Metrics (Closed Test)\n",
    "\n",
    "| Model     | Precision | Recall  | F1-score |\n",
    "|-----------|-----------|---------|----------|\n",
    "| myWord    | 0.4695    | 0.5107  | 0.4893   |\n",
    "| CRF       | 0.3186    | 0.3201  | 0.3194   |\n",
    "| LSTM      | 0.6119    | 0.6283  | **0.6200** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2cae54-3331-4e3b-83e7-8a44e86580f3",
   "metadata": {},
   "source": [
    "## Vocab Metrics (Closed Test)\n",
    "\n",
    "| Model     | Precision | Recall  | F1-score |\n",
    "|-----------|-----------|---------|----------|\n",
    "| myWord    | 0.8953    | 0.6456  | **0.7502** |\n",
    "| CRF       | 0.3256    | 0.4674  | 0.3838   |\n",
    "| LSTM      | 0.7265    | 0.7093  | 0.7178   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3f425f-67b8-4987-b750-801c3837514a",
   "metadata": {},
   "source": [
    "## Additional Evaluation Statistics (Closed Test)\n",
    "\n",
    "| **Metric**                     | **myWord** | **CRF** | **LSTM** |\n",
    "|--------------------------------|------------|---------|----------|\n",
    "| **Reference words**            | 117857     | 117857  | 117857   |\n",
    "| **Hypothesis words**           | 128206     | 118425  | 121013   |\n",
    "| **Correct words**              | 105579     | 82042   | 108546   |\n",
    "| **Reference vocab size**       | 10840      | 10840   | 10840    |\n",
    "| **Hypothesis vocab size**      | 7816       | 15563   | 10584    |\n",
    "| **Common vocabulary**          | 6998       | 5067    | 7689     |\n",
    "| **Total segmentation errors**  | 57570      | 74443   | 42500    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa7520c-d2d6-4d55-b38f-975f93ac17b1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- LU Lab. ရဲ့ word segmentation model တွေဖြစ်တဲ့ myWord ရယ်၊ CRF ရယ်၊ LSTM သုံးမျိုးကို open-test ဒေတာ စာကြောင်းရေ တစ်ထောင်၊ closed-test ဒေတာ တစ်သောင်းနဲ့ formal evaluation လုပ်ကြည့်ခဲ့တယ်။\n",
    "- သုံးမျိုးထဲမှာ ရလဒ်အကောင်းဆုံးကို ပေးနိုင်တာက LSTM ပါ\n",
    "- ဝမ်းသာဖို့ကောင်းတာက myWord က အရမ်း strong ဖြစ်တဲ့ CRF ထက် ရလဒ်ပိုကောင်းတဲ့အချက်ကိုပါ\n",
    "- ဒီ သုံးမျိုးကို baseline အနေနဲ့ ထားပြီး July 2025 မှာ အသစ်စမ်းထားတဲ့ oppaWord word segmenter နဲ့ နှိုင်းယှဉ်ကြည့်မယ်။   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae826c4d-1295-4a1a-883e-abb5c3d50aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
